{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC6hwAoCvEICgOE97ulHxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolani007/Fun-Data-Science-Content-from-Tiki/blob/main/Classification_Loss_Drill_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST: Classification Loss (with Gradient)\n",
        "\n",
        "\n",
        "Implement a NumPy-based loss_function that operates on class probabilities (softmax outputs) and integer class targets, returning both the loss value and the gradient with respect to the probabilities.\n",
        "\n",
        "\n",
        " You may use any differentiable classification loss you like.\n",
        "\n"
      ],
      "metadata": {
        "id": "2UWLU-mZ_1U3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VMvvgoo3_yIj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def loss_function(preds: np.ndarray, target: np.ndarray, reduction: str = \"mean\"):\n",
        "    # preds: [N, C] softmax probabilities (each row sums to 1)\n",
        "    # target: [N] class indices (correct class per sample)\n",
        "    # reduction: \"mean\", \"sum\", or \"none\"\n",
        "\n",
        "    n = preds.shape[0]\n",
        "    # Number of samples in the batch\n",
        "\n",
        "    # ── 1. Cross-Entropy Loss (per sample) ──\n",
        "    correct_class_probs = preds[np.arange(n), target]\n",
        "    # Pick the predicted probability of the correct class for each sample\n",
        "\n",
        "    per_sample_loss = -np.log(correct_class_probs + 1e-15)\n",
        "    # Cross-entropy: -log(p_true), epsilon avoids log(0)\n",
        "\n",
        "    # ── 2. Apply reduction ──\n",
        "    if reduction == \"mean\":\n",
        "        loss = np.mean(per_sample_loss)\n",
        "        # Average loss over the batch\n",
        "    elif reduction == \"sum\":\n",
        "        loss = np.sum(per_sample_loss)\n",
        "        # Sum loss over the batch\n",
        "    elif reduction == \"none\":\n",
        "        loss = per_sample_loss\n",
        "        # Return per-sample losses\n",
        "    else:\n",
        "        raise ValueError(\"Invalid reduction type\")\n",
        "        # Guard against invalid input\n",
        "\n",
        "    # ── 3. Gradient w.r.t. preds ──\n",
        "    grad = preds.copy()\n",
        "    # Start gradient from predicted probabilities\n",
        "\n",
        "    grad[np.arange(n), target] -= 1\n",
        "    # Subtract 1 for the correct class (softmax + CE simplification)\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        grad /= n\n",
        "        # Average gradient if loss was averaged\n",
        "\n",
        "    # ── 4. Return loss and gradient ──\n",
        "    return loss, grad\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as n\n",
        "def loss_function(preds: np.ndarray, target: np.ndarray, reduction: str = \"mean\"):\n",
        "  n = preds.shape[0]\n",
        "\n",
        "  correct_class_probs = preds[np.arange(n), target]\n",
        "  per_sample_loss = -np.log(correct_class_probs + 1e-15)\n",
        "\n",
        "  if reduction == \"mean\":\n",
        "    loss = np.mean(per_sample_loss)\n",
        "  elif reduction == 'sum':\n",
        "    loss = np.mean(per_sample_loss)\n",
        "  elif reduction == 'none':\n",
        "    loss = per_sample_loss\n",
        "    else:\n",
        "      raise ValueError('Invalid reduction type')\n",
        "\n",
        "    grad = preds.copy()\n",
        "    grad[np.arange(n), target] -= 1\n",
        "\n",
        "    if reduction == 'mean':\n",
        "      grad /= n\n",
        "\n",
        "\n",
        "    return loss, grad\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "qPnpRLb3A5ca",
        "outputId": "57b42a62-a200-4f8b-9632-bc0c1f931e1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-543537188.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-543537188.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import numpy as ndefnloss_funtion(preds: np.ndarray, target: np.ndarray, reduction: str = \"mean\"):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}