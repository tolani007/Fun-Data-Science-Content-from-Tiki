{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp8zbYy+VNSZlrtHvfdy+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolani007/Fun-Data-Science-Content-from-Tiki/blob/main/PYSPARK_FROM_N8N'S_DAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "PYSPARK STEEZE YUH\n",
        "\n",
        "80/20 principle I did not finish coding the whole documentation just 80% of it. Moving on\n",
        "\n"
      ],
      "metadata": {
        "id": "uC6cWJTEYXRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  pip install spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVy329JxYqB3",
        "outputId": "b978a96d-ca9a-44ea-c8f2-df04108baa46"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.12/dist-packages (from spark) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.22.1)\n",
            "Requirement already satisfied: docstring-parser>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.17.0)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from spark) (3.1.6)\n",
            "Requirement already satisfied: json-repair>=0.52.4 in /usr/local/lib/python3.12/dist-packages (from spark) (0.55.1)\n",
            "Requirement already satisfied: openai>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from spark) (2.15.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from spark) (2.12.3)\n",
            "Requirement already satisfied: starlette>=0.50.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.50.0)\n",
            "Requirement already satisfied: typing-extensions>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.38.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.40.0)\n",
            "Requirement already satisfied: websockets>=15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->spark) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (0.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=2.7.1->spark) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (1.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spark SQL\n",
        "!pip install pyspark[sql]\n",
        "# pandas API on Spark\n",
        "!pip install pyspark[pandas_on_spark] plotly # to plot your data, you can install plotly together.\n",
        "# Spark Connect\n",
        "!pip install pyspark[connect]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seGX0ZP2Ywja",
        "outputId": "729401bd-ff56-4f06-d10c-ddb041a2c6a5"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[sql] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[sql]) (1.17.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: pyspark[pandas_on_spark] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (2.0.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[pandas_on_spark]) (1.17.0)\n",
            "Requirement already satisfied: pyspark[connect] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (18.1.0)\n",
            "Requirement already satisfied: grpcio>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.76.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.71.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.72.0)\n",
            "Requirement already satisfied: zstandard>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (2.0.2)\n",
            "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from googleapis-common-protos>=1.71.0->pyspark[connect]) (6.33.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.76.0->pyspark[connect]) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[connect]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYSPARK_HADOOP_VERSION=3 pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnq2cfHrb5KB",
        "outputId": "b6fb9359-689f-4c13-aedc-986f2e953dca"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYSPARK_RELEASE_MIRROR=http://mirror.apache-kr.org PYSPARK_HADOOOP_VERSION=3 pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4P9xXSecicA",
        "outputId": "770e56a1-a661-4e07-9148-e4153a6e0f43"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark-connect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkvzDr3bdQqm",
        "outputId": "92baeaf6-f016-4c73-af75-ad4224c81b4d"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark-connect in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: pyspark==4.1.1 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (4.1.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (18.1.0)\n",
            "Requirement already satisfied: grpcio>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (1.76.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.71.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (1.72.0)\n",
            "Requirement already satisfied: zstandard>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.12/dist-packages (from pyspark-connect) (6.0.3)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==4.1.1->pyspark-connect) (0.10.9.9)\n",
            "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from googleapis-common-protos>=1.71.0->pyspark-connect) (6.33.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.76.0->pyspark-connect) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyspark-connect) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyspark-connect) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyspark-connect) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyspark-connect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n pyspark_env\n",
        "!conda activate pyspark_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z373Rfa0UxU0",
        "outputId": "1f0a7311-034c-4e39-9cb8-2941dc8dbe51"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJq9J7mfiir0",
        "outputId": "0078f7cc-9d21-4cc6-d717-eb07c5f705ae"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "manual download of spark"
      ],
      "metadata": {
        "id": "5eKxsJTzb-Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf spark-\\ |release|\\-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEd4vOX1joes",
        "outputId": "7859d699-67c7-4eb2-83cc-2b6cbbeec6a2"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: -bin-hadoop3.tgz: command not found\n",
            "/bin/bash: line 1: release: command not found\n",
            "tar (child): spark- : Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User needs to replace {release_version} with the actual Spark version (e.g., 3.5.1)\n",
        "!cd spark-{release_version}-bin-hadoop3 && \\\n",
        "export SPARK_HOME=$(pwd) && \\\n",
        "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtX8eLPzmRuq",
        "outputId": "f18f2922-0a69-4c96-d81a-506c707cbec4"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: spark-{release_version}-bin-hadoop3: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyspark[connect]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thHZV1vqoTHj",
        "outputId": "9a64088d-6079-4935-9996-8952e576bc3e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[connect] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (18.1.0)\n",
            "Requirement already satisfied: grpcio>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.76.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.71.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (1.72.0)\n",
            "Requirement already satisfied: zstandard>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[connect]) (2.0.2)\n",
            "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from googleapis-common-protos>=1.71.0->pyspark[connect]) (6.33.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.76.0->pyspark[connect]) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[connect]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[connect]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyspark[sql]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6dUbvKqos-G",
        "outputId": "57c91b07-d32b-48fa-b115-27b3e1683989"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[sql] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[sql]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[sql]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[sql]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flameprof: Provide the default renderer for UDF performance profiling"
      ],
      "metadata": {
        "id": "VofQ6tPyo75G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PANDAS API ON SPARK"
      ],
      "metadata": {
        "id": "RqSfbE_6pUND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyspark[pandas_on_spark]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCBcxJbRpHVt",
        "outputId": "e81bd19d-b92c-4986-f223-f9aad3f58943"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[pandas_on_spark] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[pandas_on_spark]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pandas_on_spark]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[pandas_on_spark]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "!pip install mlflow\n",
        "import pyspark.pandas as ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl6Qqs0Gphwn",
        "outputId": "7f176517-410f-4a6e-dd25-4011d87f7e83"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (3.9.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.9.0)\n",
            "Requirement already satisfied: mlflow-tracing==3.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.9.0)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow) (6.0.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.18.1)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.12/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: huey<3,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.6.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: skops<1 in /usr/local/lib/python3.12/dist-packages (from mlflow) (0.13.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.46)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.2.5)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.2)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.82.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.33.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.5.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.5)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.7)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: prettytable>=3.9 in /usr/local/lib/python3.12/dist-packages (from skops<1->mlflow) (3.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (3.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable>=3.9->skops<1->mlflow) (0.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (2026.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow) (4.12.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.6.2)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (3.9.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.9.0)\n",
            "Requirement already satisfied: mlflow-tracing==3.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.9.0)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow) (6.0.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.18.1)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.12/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: huey<3,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.6.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: skops<1 in /usr/local/lib/python3.12/dist-packages (from mlflow) (0.13.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.46)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.2.5)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.2)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.82.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.33.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.5.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.5)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.7)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: prettytable>=3.9 in /usr/local/lib/python3.12/dist-packages (from skops<1->mlflow) (3.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (3.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable>=3.9->skops<1->mlflow) (0.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (2026.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow) (4.12.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark \"pyspark [ml]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyGIR0YtrdEE",
        "outputId": "8c0bc8be-6b0d-466e-80a1-3c528fd377ed"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[ml]) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "additional libraries that enhance functionality but are not included in the installation packages\n",
        "\n",
        "scipy: Required for scipy integration\n",
        "scikit-learn: Required for implementing machine learning algorithms\n",
        "\n",
        "torch: required for machine learning modeling\n",
        "\n",
        "torchvision: Required for supporting image and video processing\n",
        "\n",
        "torcheval: Required  for model evaluation\n",
        "deepspeed: is for high_performance model training optimization. INstallable on non=Darwin systems"
      ],
      "metadata": {
        "id": "YVmRBw-crzEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLlib\n",
        "Installable with !pip install \"pyspark[mllib]\""
      ],
      "metadata": {
        "id": "r0HBLNZtzNtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyspark[mllib]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RkSyUcGzY8c",
        "outputId": "c659e9bf-bc85-43c8-a1bb-e05e86b4d163"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[mllib] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[mllib]) (0.10.9.9)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[mllib]) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pyspark[pipelines]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5z7N_sKzgey",
        "outputId": "b3b5d657-1864-4db4-dc66-f8db36568850"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark[pipelines] in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (0.10.9.9)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (2.0.2)\n",
            "Requirement already satisfied: grpcio>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status>=1.76.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (1.76.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.71.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (1.72.0)\n",
            "Requirement already satisfied: zstandard>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (0.25.0)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.12/dist-packages (from pyspark[pipelines]) (6.0.3)\n",
            "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from googleapis-common-protos>=1.71.0->pyspark[pipelines]) (6.33.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.76.0->pyspark[pipelines]) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pipelines]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pipelines]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->pyspark[pipelines]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->pyspark[pipelines]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quickstart: DataFrame"
      ],
      "metadata": {
        "id": "zWQTH-Gzc3ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO __init__ PySpark import SparkSession"
      ],
      "metadata": {
        "id": "XvgjtD2JzeOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "hOWyqXvj3zDZ"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#create PySpark DataFrame from a list of rows\n",
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=1, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
        "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "AoVUEpBg4XQ0",
        "outputId": "0291b380-8219-445a-d2d8-a9c5d521eddb"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownException",
          "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle a8b7055e-d1d7-4604-8636-d7643ca99cdb is invalid. Session was closed. SQLSTATE: HY000",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3271260040.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m df = spark.createDataFrame([\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/connect/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Get all related configs in a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         configs = self._client.get_config_dict(\n\u001b[0m\u001b[1;32m    536\u001b[0m             \u001b[0;34m\"spark.sql.timestampType\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;34m\"spark.sql.session.timeZone\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/connect/client/core.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(self, *keys)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigRequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigRequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m     def get_config_with_defaults(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/connect/client/core.py\u001b[0m in \u001b[0;36mconfig\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1798\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSparkConnectException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid state during retry exception handling.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     def _interrupt_request(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/connect/client/core.py\u001b[0m in \u001b[0;36m_handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1980\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_error_handling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_rpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1983\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/connect/client/core.py\u001b[0m in \u001b[0;36m_handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         )\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                     raise convert_exception(\n\u001b[0m\u001b[1;32m   2067\u001b[0m                         \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m                         \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownException\u001b[0m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle a8b7055e-d1d7-4604-8636-d7643ca99cdb is invalid. Session was closed. SQLSTATE: HY000"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Pyspark Dataframe with an explicit schema"
      ],
      "metadata": {
        "id": "MgwxWoTyDkW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "    (1,2., 'string', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3, \"string\", date(2000, 3, 1), datetime(2000, 1, 3, 12, 0 ))\n",
        "\n",
        "], schema= 'a long, b double, c string, d date, e timestamp')\n",
        "df"
      ],
      "metadata": {
        "id": "qsM46kDGCGjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Pyspark DataFrame from a pandas DataFrame"
      ],
      "metadata": {
        "id": "gTguAsVgE2o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    \"a\": [1, 2, 3],\n",
        "    \"b\": [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n"
      ],
      "metadata": {
        "id": "_4TEQVmeE1HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "XwHYRNXlG9_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "pNwF2YyxHsA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "_J0sW3RHH8Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(3)"
      ],
      "metadata": {
        "id": "pHn96I4MIBvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "df"
      ],
      "metadata": {
        "id": "yjzqCsCcIGVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "_bFYKxxxIwnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "moldhzpxJDAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "FRheZlKndRVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('a', 'b', 'c').describe().show()"
      ],
      "metadata": {
        "id": "c_-8oITqd2rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect"
      ],
      "metadata": {
        "id": "YkwVV93zeMjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(1)"
      ],
      "metadata": {
        "id": "EhQ5LcAdePfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.toPandas()"
      ],
      "metadata": {
        "id": "dZnHn8U_eR32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.a"
      ],
      "metadata": {
        "id": "KfAuzvH5d8D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
      ],
      "metadata": {
        "id": "d8NdP86igepf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.c).show()"
      ],
      "metadata": {
        "id": "1zo0ThkShD3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('upper_c', upper(df.c)).show()"
      ],
      "metadata": {
        "id": "dd2nR5L6hJVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('upper_c', upper(df.c)). show()"
      ],
      "metadata": {
        "id": "gk0X4ZZ9hYz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.a == 1).show()"
      ],
      "metadata": {
        "id": "6z-cCNAJh363"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying a function"
      ],
      "metadata": {
        "id": "Xu2kSRElh_YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "  #Simply plus one by using pandas Series.\n",
        "  return series + 1\n",
        "df.select(pandas_plus_one(df.a)).show()"
      ],
      "metadata": {
        "id": "OS0GUFGtiB-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_filter_func(iterator):\n",
        "  for pandas_df in iterator:\n",
        "    yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema= df.schema).show()"
      ],
      "metadata": {
        "id": "lg9Wb1MbjLlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "grouping data\n",
        "\n",
        "approach for grouping data in pyspark =\n",
        "split-apply-combine strategy"
      ],
      "metadata": {
        "id": "qNUdUXHfjvOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['purple', 'pomegranate', 3, 15], ['sky-blue', 'pear', 7, 10000], ['green','apple', 1, 25],\n",
        "    ['lavender', 'pineapple', 8, 10], ['yellow', 'orange', 9, 27], ['black', 'diced pineapples', 7, 9 ]\n",
        "])\n",
        "df.show"
      ],
      "metadata": {
        "id": "j2_Kot7VkJGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('_1').avg().show()"
      ],
      "metadata": {
        "id": "YYdfaJCPdg9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "wMcd4PSjeekc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plus_mean(pandas_df):\n",
        "  return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
        "  df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n"
      ],
      "metadata": {
        "id": "btwrKZIud4Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "co-grouping and applying a function"
      ],
      "metadata": {
        "id": "N9_texuAfAbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (2000101, 2, 2.0), (20000102, 1, 3.0), (2000102, 2,4.0)],\n",
        "    ('time', 'id', 'v1'))\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, 'x'), (20000101, 2, 'y')], ('time', 'id', 'v2'))\n",
        "\n",
        "\n",
        "def merge_ordered(l, r):\n",
        "  return pd.merge_ordered(l,r)\n",
        "\n",
        "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
        "    merge_ordered, schema= 'time int, id int, v1 double, v2 string').show()\n",
        "\n"
      ],
      "metadata": {
        "id": "E0iP7A05fEk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV"
      ],
      "metadata": {
        "id": "SW3C-ax9ioYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.csv('foo.csv', header=True, mode='overwrite')\n",
        "spark.read.csv('foo.csv', header=True).show()"
      ],
      "metadata": {
        "id": "AOxn9t4bim7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parquet"
      ],
      "metadata": {
        "id": "e-sHSQeZk7Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet('bar.parquet', mode='overwrite')\n",
        "spark.read.parquet('bar.parquet').show()"
      ],
      "metadata": {
        "id": "q7H4dNtUk6Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ORC"
      ],
      "metadata": {
        "id": "Mu8LnMBRlyk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.orc('zoo.orc')\n",
        "spark.read.orc('zoo.orc').show()"
      ],
      "metadata": {
        "id": "IoG3q_g8l1ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with SQL"
      ],
      "metadata": {
        "id": "fD9G0X2QmEe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('tableA')\n",
        "spark.sql('SELECT count(*) from tableA').show()"
      ],
      "metadata": {
        "id": "JAwxJXbamGso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN addotion, UDFS can be registered and invoked in sql out of the box\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "PoSlfWqOmgwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf('integer')\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "  return s + 1\n",
        "spark.udf.register('add_one', add_one)\n",
        "spark.sql('SELECT add_one(_3) FROM tableA').show()"
      ],
      "metadata": {
        "id": "lqR6b_timo7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These SQL expressions can directly be mixed and nused as PySpark columns."
      ],
      "metadata": {
        "id": "QPajw-yIqYTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "df.selectExpr('add_one(_3)').show()\n",
        "df.select(expr('count(*)') > 0).show()"
      ],
      "metadata": {
        "id": "iGZpLFrKqh4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas API on Spark\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "5B45r3hDyYQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark.pandas as ps\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "pKFBHil6rGVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object Creation\n",
        "\n",
        "Creating a pandas-on-Spark Series by passing a list of values, letting pandas API on Spark create a default integer index:"
      ],
      "metadata": {
        "id": "NVeVGZleyuL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = ps.Series([1, 3, 5, np.nan, 6, 8])"
      ],
      "metadata": {
        "id": "EpR_xiotzWdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "id": "p39YSukmzdvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a pandas-on-Spark DataFrame by passing a dict of objects that can be converted to series-like."
      ],
      "metadata": {
        "id": "hGehJa1jzlCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.DataFrame(\n",
        "    {\n",
        "        'a': [1, 2, 3, 4, 5, 6],\n",
        "        'b': [100, 200, 300, 400, 500, 600],\n",
        "        \"c\": [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
        "        index = [10, 20, 30, 40, 50, 60]\n",
        ")"
      ],
      "metadata": {
        "id": "ayk_pSN8zxqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf"
      ],
      "metadata": {
        "id": "3IMC1pzV0-uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a pandas DataFrame by passing a numpy array, with a datetime index and labeled columns:"
      ],
      "metadata": {
        "id": "5F5dzXVA1BqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = pd.date_range('20130101', periods=6)"
      ],
      "metadata": {
        "id": "FfJQEmmp1Q1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates"
      ],
      "metadata": {
        "id": "1l4PJAWJ1dk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns= list('ABCD'))"
      ],
      "metadata": {
        "id": "DlWMHc9q1dGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf"
      ],
      "metadata": {
        "id": "2_nkziK815nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this pandas DataFrame can be converted to a pandas-on-Spark DataFrame."
      ],
      "metadata": {
        "id": "fc9ABXmn3j4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(pdf)"
      ],
      "metadata": {
        "id": "7RTOGtMS30fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(psdf)"
      ],
      "metadata": {
        "id": "PPjZTeJx3516"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating s spark  DdataFrame  from pandas DataFrame"
      ],
      "metadata": {
        "id": "WXVA_LBCKisS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "Cw8-LWxAKJNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = spark.createDataFrame(pdf)"
      ],
      "metadata": {
        "id": "XtVnrc7RKVnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.show()"
      ],
      "metadata": {
        "id": "PJvuw0PhKbzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating pandas-on-Spark DataFrame from Spark DataFrame"
      ],
      "metadata": {
        "id": "-_RQPXsKK2UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = sdf.pandas_api()"
      ],
      "metadata": {
        "id": "f5QUTyp_LGoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf"
      ],
      "metadata": {
        "id": "TsuasKfcLKHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.dtypes"
      ],
      "metadata": {
        "id": "Jm4oNTJOLVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf"
      ],
      "metadata": {
        "id": "iJ13oetCLvEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.index"
      ],
      "metadata": {
        "id": "2shQ6YBVL0Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.columns"
      ],
      "metadata": {
        "id": "HssUlp34L5z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.columns"
      ],
      "metadata": {
        "id": "fuDP7V3YMIwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.to_numpy()"
      ],
      "metadata": {
        "id": "HcmyrrAwMQtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.describe()"
      ],
      "metadata": {
        "id": "PGL1sdIMMauq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transposing your data"
      ],
      "metadata": {
        "id": "QXU5NvP9M47N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.T"
      ],
      "metadata": {
        "id": "QT9V2qhoM7-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting by its index"
      ],
      "metadata": {
        "id": "3C4TJD33NBKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.sort_index(ascending= False)"
      ],
      "metadata": {
        "id": "BBnXjefqNFso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting by value"
      ],
      "metadata": {
        "id": "_73pZjzlNV8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.sort_values(by='B')"
      ],
      "metadata": {
        "id": "yCWV3oc8NXqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing data\n",
        "---\n",
        "Pandas API on Spark primarily uses the value np.nan to represent missing data. It is by default not inclused in computation"
      ],
      "metadata": {
        "id": "vD-GPH1cOqYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf1 = pdf.reindex(index=dates[0:4], columns=list(pdf.columns) + ['E'])"
      ],
      "metadata": {
        "id": "-_UR-eXMOplm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf1.loc[dates[0]:dates[1], 'E'] = 1"
      ],
      "metadata": {
        "id": "Q9XisqD2Pi89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1 = ps.from_pandas(pdf1)"
      ],
      "metadata": {
        "id": "VsE8OR9xPsxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1"
      ],
      "metadata": {
        "id": "ymqplEumPx1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To drop any rows that have misssing data"
      ],
      "metadata": {
        "id": "1kI5L5vrQAJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1.fillna(value=5)"
      ],
      "metadata": {
        "id": "cJppUwyCQNhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To drop any rows  that have missing data"
      ],
      "metadata": {
        "id": "nu9KkQXiQiYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1.dropna(how= 'any')"
      ],
      "metadata": {
        "id": "JTKSAgo-RAuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fill in missing data"
      ],
      "metadata": {
        "id": "yqRVjAuFRIQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1.fillna(value=5)"
      ],
      "metadata": {
        "id": "oFcVRq8MQov3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operations"
      ],
      "metadata": {
        "id": "ocYqcVdcdrzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stats\n",
        "## Performing a descriptive statistics"
      ],
      "metadata": {
        "id": "thLEZBZOdydQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.mean()"
      ],
      "metadata": {
        "id": "QdwHvpq3d7n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark Configurations\n",
        "\n",
        "### Configs in pyspark can me applied in pandas APi on Spark.\n",
        "\n",
        "enable Arrow optimization to hugely speed up internal pandas conversion.\n",
        "\n",
        "PySpark Usage Guide for Pandas with Apache Arrow in PySpark documentation\n",
        "\n"
      ],
      "metadata": {
        "id": "8H9s8YSZd_oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prev = spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')\n",
        "ps.set_option('compute.default_index_type', 'distributed')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #ignore warnings coming from Arrow optimizations"
      ],
      "metadata": {
        "id": "CuMT4h17RM_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
        "%timeit ps.range(300000).to_pandas()"
      ],
      "metadata": {
        "id": "0khGH7tZfvJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
        "%timeit ps.range(300000).to_pandas()"
      ],
      "metadata": {
        "id": "qyqlcneziOlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps.reset_option('compute.default_index_type')\n",
        "spark.conf.set('spark.sql.executiojn.arrow.pyspark.enabled', prev)"
      ],
      "metadata": {
        "id": "1dQ020SYiyfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping:\n",
        "group based splitting\n",
        "\n",
        "independent group function application\n",
        "\n",
        "data results combination"
      ],
      "metadata": {
        "id": "Ab3D5dN92Yul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.DataFrame(\n",
        "    {\n",
        "        'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n",
        "        'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n",
        "        'C': np.random.randn(8),\n",
        "        'D': np.random.randn(8)\n",
        "    })\n",
        "\n",
        "#output error because spark session expired but clean code nonetheless"
      ],
      "metadata": {
        "id": "8YvwUmqb2yyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf"
      ],
      "metadata": {
        "id": "Pj8_DUA033SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping and then applying the sum() function to the resulting groups"
      ],
      "metadata": {
        "id": "RKJk-TICM7Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.groupby('A').sum()"
      ],
      "metadata": {
        "id": "E75Fca0bNMh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping by multiple columns forms a heirarchical index , and again we can apply the sum function"
      ],
      "metadata": {
        "id": "2ld_y-pNHsuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.groupby(['A', 'B']).sum()"
      ],
      "metadata": {
        "id": "sYNhxhk0IB8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting"
      ],
      "metadata": {
        "id": "EdT3jcRMIOct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pser= pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))"
      ],
      "metadata": {
        "id": "IegancXdIQSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psser = ps.Series(pser)"
      ],
      "metadata": {
        "id": "b93rwP4HIya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psser = psser.cummax()"
      ],
      "metadata": {
        "id": "77lQKlo3I176"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psser.plot()"
      ],
      "metadata": {
        "id": "SwziQQRZI5h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pd.DataFrame(np.random.randn(1000, 4), index=pser.index, columns=['A', 'B', 'C', 'D'])"
      ],
      "metadata": {
        "id": "5JVYPmdFK8oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(pdf)"
      ],
      "metadata": {
        "id": "kByyyk3lLLd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = psdf.cummax()"
      ],
      "metadata": {
        "id": "R1PUyQV4LO0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.plot()"
      ],
      "metadata": {
        "id": "QXgwV3htLSGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting data in/out\n",
        "## CSV\n",
        "### CSV is easy to use."
      ],
      "metadata": {
        "id": "K-j2jW4TKgmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.to_csv('foo.csv')\n",
        "ps.read_csv('foo.csv').head(10)"
      ],
      "metadata": {
        "id": "4niiuDHKKwot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parquet\n",
        "\n",
        "## Parquet is an effficient and compact file format to read and write faster.\n",
        "\n",
        "### Parquet is faster to deal with csv"
      ],
      "metadata": {
        "id": "F88Whah_N4nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.to_parquet('bar.parquet')\n",
        "ps.read_parquet('bar.parquet').head(10)"
      ],
      "metadata": {
        "id": "-jUm_q5mOXW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark IO\n",
        "\n",
        "## pandas API on Spark fully supports Spark's various datasources such as ORC and external datasource.\n",
        "\n",
        "## ORC is datasource of Spark"
      ],
      "metadata": {
        "id": "RuTkLBUtOmGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.spark.to_spark_io('zoo.orc', format='orc')\n",
        "ps.read_spark_io('zoo.orc', format='orc').head(10)"
      ],
      "metadata": {
        "id": "9WnRMnURO_fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing PySpark\n",
        "\n",
        "## Build a PySpark Application\n"
      ],
      "metadata": {
        "id": "gHHrNiC_UAGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start spark Session"
      ],
      "metadata": {
        "id": "6xCrYq4jUMQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#Create a SparkSession\n",
        "spark = SparkSession.builder.appName('Testing PySpark Example').getOrCreate()"
      ],
      "metadata": {
        "id": "pk_FNnlZUWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create my DataFrame"
      ],
      "metadata": {
        "id": "xR8cl7R_U0JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [{'name': 'Eigentiki.', 'age': 27},\n",
        "               {'name': 'Tiki', 'age': '23'},\n",
        "               {'name': \"Tikky\", 'age': '18'},\n",
        "               {'name': \"Tolani Akinola\", 'age': 'INFINITY'}\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "df = spark.createDataFrame(sample_data)"
      ],
      "metadata": {
        "id": "S9JFJtL4U4Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "zS21BZV6V37e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now lets define and apply a transformation function on my fucking dataframe"
      ],
      "metadata": {
        "id": "V3MBkoWiV9Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace\n",
        "\n",
        "\n",
        "#Remove additional spaces in name\n",
        "def remove_extra_spaces(df, column_name):\n",
        "  #Remoce extra spaces from the specified columns\n",
        "  df_transformed = df.withColumn(column_name, regexp_replace(col(column_name),  \"\\\\s+\", ''))\n",
        "  return df_transformed\n",
        "\n",
        "\n",
        "\n",
        "transformed_df = remove_extra_spaces(df, 'name')\n",
        "transformed_df.show()"
      ],
      "metadata": {
        "id": "UPYWAu7vWHD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing your Pyspark Application\n",
        "\n",
        "#eyeball the resulting frame\n",
        "\n",
        "\n",
        "#eyeballing is impractical for big ass datasets\n",
        "\n",
        "# ad-hoc validation cases = unplanned and quick tests\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6GIYbgxXcu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is how I assert equality bettwen two dataframes:"
      ],
      "metadata": {
        "id": "gvVb7EAurifx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.testing\n",
        "from pyspark.testing.utils import assertDataFrameEqual\n",
        "#Example 1\n",
        "df1 = spark.createDataFrame(data=[('1', 1000), ('2', 3000)], schema=['id', 'amount'])\n",
        "df2 = spark.createDataFrame(data=[('1', 1000), ('2', 3000)],  schema=['id', 'amount'])\n",
        "assertDataFrameEqual(df1, df2)"
      ],
      "metadata": {
        "id": "1nyC4MjTV5W6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}